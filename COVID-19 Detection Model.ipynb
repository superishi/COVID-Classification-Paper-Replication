{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying COVID-19 datasets based on routine blood examination\n",
    "\n",
    "Replicating the ML models in the paper “Development, evaluation, and validation of machine learning models\n",
    "for COVID-19 detection based on routine blood tests”. The research paper and the dataset can be\n",
    "\n",
    "Aim is to replicate the ML models as described in the research paper by applying all steps the paper\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for ML Model Building\n",
    "1. Discard all features with a missing data rate greater than 75%\n",
    "2. Data imputation using multivariate k-nearest neighbour algorithm with k = 5\n",
    "3. Feature selection using recursive feature-elimination algorithm, optimal features selected through hyper-parameter optimization\n",
    "4. Evaluate classification algorithms:\n",
    "    - Random Forest\n",
    "    - Naive Bayes \n",
    "    - Logistic Regression\n",
    "    - Support Vector Machine\n",
    "    - k-Nearest Neighbours\n",
    "5. Tune hyperparameters using grid search approach\n",
    "\n",
    "\n",
    "## For Model Selection\n",
    "1. Split data set into training set (80%) and test set (20%) using stratified approach\n",
    "2. Hyper-parameter optimization using 5-fold stratified cross-validation grid search using AUC as reference measure\n",
    "3. Train and calibrate on entire training set\n",
    "4. Evaluate on test set using measures of accuracy, sensitivity, specificity, AUC and Brier score\n",
    "5. For each model class, consider the standard version and a 3-way version (model that abstains from predictions where confidence score is below 75%)\n",
    "\n",
    "Additionally, control randomisation in all stages of model development to ensure repeatability.\n",
    "\n",
    "Libraries utilised:\n",
    "- numpy\n",
    "- pandas\n",
    "- scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1736, 36)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>CA</th>\n",
       "      <th>CK</th>\n",
       "      <th>CREA</th>\n",
       "      <th>ALP</th>\n",
       "      <th>GGT</th>\n",
       "      <th>GLU</th>\n",
       "      <th>AST</th>\n",
       "      <th>...</th>\n",
       "      <th>MO</th>\n",
       "      <th>EO</th>\n",
       "      <th>BA</th>\n",
       "      <th>NET</th>\n",
       "      <th>LYT</th>\n",
       "      <th>MOT</th>\n",
       "      <th>EOT</th>\n",
       "      <th>BAT</th>\n",
       "      <th>Suspect</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A00345_2020-03-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.150</td>\n",
       "      <td>95.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.40</td>\n",
       "      <td>1.20</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A00791_2020-03-19</td>\n",
       "      <td>1.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1.97</td>\n",
       "      <td>237.0</td>\n",
       "      <td>0.970</td>\n",
       "      <td>54.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A00741_2020-03-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2.11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>147.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.45</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A00605_2020-04-15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.27</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0.755</td>\n",
       "      <td>123.5</td>\n",
       "      <td>176.5</td>\n",
       "      <td>106.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>3.60</td>\n",
       "      <td>2.60</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A00417_2020-02-24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.07</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1.810</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>49</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.38</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.800</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.960</td>\n",
       "      <td>79.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>51</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.40</td>\n",
       "      <td>124.0</td>\n",
       "      <td>0.950</td>\n",
       "      <td>48.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1735</th>\n",
       "      <td>53</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.040</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1736 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Unnamed: 0  Sex   Age    CA     CK   CREA    ALP    GGT    GLU  \\\n",
       "0     A00345_2020-03-25  1.0  82.0  2.09    NaN  1.150   95.0   40.0   78.0   \n",
       "1     A00791_2020-03-19  1.0  51.0  1.97  237.0  0.970   54.0   98.0   98.0   \n",
       "2     A00741_2020-03-04  1.0  58.0  2.11    NaN  1.000   80.0  147.0  106.0   \n",
       "3     A00605_2020-04-15  0.0  82.0  2.27  138.0  0.755  123.5  176.5  106.0   \n",
       "4     A00417_2020-02-24  1.0  79.0  2.07   73.0  1.810   62.0   36.5   96.0   \n",
       "...                 ...  ...   ...   ...    ...    ...    ...    ...    ...   \n",
       "1731                 49  0.0   NaN  2.38   40.0  0.800   68.0    9.0  128.0   \n",
       "1732                 50  0.0   NaN  2.36    NaN  0.960   79.0   35.0  107.0   \n",
       "1733                 51  1.0   NaN  2.28    NaN  1.420    NaN    NaN  136.0   \n",
       "1734                 52  1.0   NaN  2.40  124.0  0.950   48.0   44.0   95.0   \n",
       "1735                 53  1.0   NaN  2.14    NaN  1.040    NaN    NaN  179.0   \n",
       "\n",
       "        AST  ...    MO   EO   BA   NET   LYT  MOT  EOT  BAT  Suspect  target  \n",
       "0      26.0  ...   9.5  2.9  0.5  6.40  1.20  0.8  0.3  0.0      1.0       0  \n",
       "1      74.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       1  \n",
       "2      41.0  ...   7.3  0.3  0.1  5.45  0.75  0.5  0.0  0.0      1.0       0  \n",
       "3     114.0  ...   9.5  1.7  0.9  3.60  2.60  0.7  0.1  0.1      0.5       0  \n",
       "4      28.0  ...  10.0  8.5  0.5  0.40  0.50  0.1  0.1  0.0      1.0       0  \n",
       "...     ...  ...   ...  ...  ...   ...   ...  ...  ...  ...      ...     ...  \n",
       "1731   13.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1732   24.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1733   53.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1734   50.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "1735   28.0  ...   NaN  NaN  NaN   NaN   NaN  NaN  NaN  NaN      1.0       0  \n",
       "\n",
       "[1736 rows x 36 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(r'C:\\Users\\Cindy\\Downloads\\Trial Task Material Extracted\\Resources (PIT-Trial Task)\\SARS-CoV-2\\all_training.csv')\n",
    "is_altered = False\n",
    "print(dataset.shape)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1736, 32)\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.dropna(axis = 1, thresh = int(0.75 * dataset.shape[0]))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 82.0, 2.09, ..., 0.0, 1.0, 0],\n",
       "       [1.0, 51.0, 1.97, ..., nan, 1.0, 1],\n",
       "       [1.0, 58.0, 2.11, ..., 0.0, 1.0, 0],\n",
       "       ...,\n",
       "       [1.0, nan, 2.28, ..., nan, 1.0, 0],\n",
       "       [1.0, nan, 2.4, ..., nan, 1.0, 0],\n",
       "       [1.0, nan, 2.14, ..., nan, 1.0, 0]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if type(dataset) is not np.ndarray:\n",
    "    dataset = dataset.to_numpy()\n",
    "    \n",
    "\n",
    "if is_altered is False:\n",
    "    dataset = dataset[:,1:]\n",
    "    dataset\n",
    "    is_altered = True\n",
    "    \n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting Training and Test Data with Straitification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dataset[:,:-1]\n",
    "targets = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.2, random_state = 123, stratify = targets)\n",
    "\n",
    "y_train = y_train.astype('int')\n",
    "y_test = y_test.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 11.0 2.365 ... nan nan 0.0]\n",
      " [0.0 60.0 2.22 ... 0.3 0.0 0.0]\n",
      " [1.0 47.0 nan ... nan nan 0.0]\n",
      " ...\n",
      " [1.0 46.0 2.28 ... nan nan 0.5]\n",
      " [1.0 86.0 2.19 ... 0.0 0.0 1.0]\n",
      " [1.0 76.0 2.0022375 ... 0.15 0.016 1.0]]\n",
      "[0 0 0 ... 0 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 1.1000000e+01, 2.3650000e+00, ..., 1.3200000e-01,\n",
       "        6.0000000e-03, 0.0000000e+00],\n",
       "       [0.0000000e+00, 6.0000000e+01, 2.2200000e+00, ..., 3.0000000e-01,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       [1.0000000e+00, 4.7000000e+01, 2.4140000e+00, ..., 1.0000000e-02,\n",
       "        0.0000000e+00, 0.0000000e+00],\n",
       "       ...,\n",
       "       [1.0000000e+00, 4.6000000e+01, 2.2800000e+00, ..., 2.6000000e-01,\n",
       "        0.0000000e+00, 5.0000000e-01],\n",
       "       [1.0000000e+00, 8.6000000e+01, 2.1900000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00],\n",
       "       [1.0000000e+00, 7.6000000e+01, 2.0022375e+00, ..., 1.5000000e-01,\n",
       "        1.6000000e-02, 1.0000000e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train)\n",
    "\n",
    "imputer = KNNImputer(n_neighbors = 5, weights = \"uniform\")\n",
    "\n",
    "X_train = imputer.fit_transform(X_train)\n",
    "X_test = imputer.transform(X_test)\n",
    "\n",
    "print(y_train)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparam_selection(min_range, max_range):    \n",
    "    optimal_feature_num = 30\n",
    "    max_accuracy = 0\n",
    "\n",
    "    for num_features in range(min_range, max_range + 1):\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select = num_features)\n",
    "        model = DecisionTreeClassifier()\n",
    "        pipeline = Pipeline(steps=[('s',rfe),('m',model)])\n",
    "        X_train_copy = rfe.fit_transform(X_train, y_train)\n",
    "\n",
    "        #evaluation\n",
    "        cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "        n_scores = cross_val_score(pipeline, X_test, y_test, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')\n",
    "        print('Number of features: %d   Accuracy: %.3f (%.3f)' % (num_features, mean(n_scores), std(n_scores)))\n",
    "\n",
    "        if mean(n_scores) > max_accuracy:\n",
    "            max_accuracy = mean(n_scores)\n",
    "            optimal_feature_num = num_features\n",
    "    \n",
    "    return optimal_feature_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1   Accuracy: 0.591 (0.094)\n",
      "Number of features: 2   Accuracy: 0.677 (0.075)\n",
      "Number of features: 3   Accuracy: 0.708 (0.082)\n",
      "Number of features: 4   Accuracy: 0.729 (0.072)\n",
      "Number of features: 5   Accuracy: 0.716 (0.077)\n",
      "Number of features: 6   Accuracy: 0.720 (0.064)\n",
      "Number of features: 7   Accuracy: 0.718 (0.064)\n",
      "Number of features: 8   Accuracy: 0.737 (0.064)\n",
      "Number of features: 9   Accuracy: 0.746 (0.066)\n",
      "Number of features: 10   Accuracy: 0.752 (0.054)\n",
      "Number of features: 11   Accuracy: 0.741 (0.065)\n",
      "Number of features: 12   Accuracy: 0.742 (0.080)\n",
      "Number of features: 13   Accuracy: 0.750 (0.062)\n",
      "Number of features: 14   Accuracy: 0.749 (0.076)\n",
      "Number of features: 15   Accuracy: 0.742 (0.073)\n",
      "Number of features: 16   Accuracy: 0.741 (0.072)\n",
      "Number of features: 17   Accuracy: 0.742 (0.063)\n",
      "Number of features: 18   Accuracy: 0.743 (0.072)\n",
      "Number of features: 19   Accuracy: 0.736 (0.076)\n",
      "Number of features: 20   Accuracy: 0.737 (0.071)\n",
      "Number of features: 21   Accuracy: 0.753 (0.075)\n",
      "Number of features: 22   Accuracy: 0.741 (0.079)\n",
      "Number of features: 23   Accuracy: 0.746 (0.074)\n",
      "Number of features: 24   Accuracy: 0.739 (0.080)\n",
      "Number of features: 25   Accuracy: 0.744 (0.073)\n",
      "Number of features: 26   Accuracy: 0.744 (0.075)\n",
      "Number of features: 27   Accuracy: 0.746 (0.078)\n",
      "Number of features: 28   Accuracy: 0.729 (0.068)\n",
      "Number of features: 29   Accuracy: 0.753 (0.065)\n",
      "Number of features: 30   Accuracy: 0.734 (0.080)\n"
     ]
    }
   ],
   "source": [
    "opt_feature_num = hyperparam_selection(1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "(1388, 30)\n"
     ]
    }
   ],
   "source": [
    "print(opt_feature_num)\n",
    "print(X_train.shape)\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select = opt_feature_num)\n",
    "X_train = rfe.fit_transform(X_train, y_train)\n",
    "X_test = rfe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1388, 21)\n",
      "(348, 21)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.74000000e+01 1.84000000e+00 1.21000000e+00 ... 5.13333334e-02\n",
      "  3.55555560e-03 1.00000000e+00]\n",
      " [3.20000000e+01 2.05000000e+00 8.30000000e-01 ... 0.00000000e+00\n",
      "  0.00000000e+00 1.00000000e+00]\n",
      " [5.60000000e+01 2.32000000e+00 9.80000000e-01 ... 1.00000000e-01\n",
      "  1.00000000e-01 5.00000000e-01]\n",
      " ...\n",
      " [4.70000000e+01 2.48000000e+00 5.30000000e-01 ... 3.00000000e-01\n",
      "  1.00000000e-01 1.00000000e+00]\n",
      " [7.70000000e+01 2.08332500e+00 7.70000000e-01 ... 4.00000000e-02\n",
      "  6.00000000e-03 0.00000000e+00]\n",
      " [7.30000000e+01 2.17896667e+00 5.70000000e-01 ... 1.06666667e-01\n",
      "  3.33333330e-02 1.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.67640729, -2.18664511,  0.06475289, ..., -0.14971628,\n",
       "        -0.34798244,  0.63517156],\n",
       "       [-1.45985134, -0.93814126, -0.34414052, ..., -0.55905691,\n",
       "        -0.44518681,  0.63517156],\n",
       "       [-0.23889958,  0.66707797, -0.18273523, ...,  0.2383599 ,\n",
       "         2.28868624, -0.68659474],\n",
       "       ...,\n",
       "       [-0.69675649,  1.618319  , -0.66695111, ...,  1.83319353,\n",
       "         2.28868624,  0.63517156],\n",
       "       [ 0.82943321, -0.74001559, -0.40870264, ..., -0.24009019,\n",
       "        -0.28115443, -2.00836104],\n",
       "       [ 0.62594125, -0.17140136, -0.6239097 , ...,  0.29152102,\n",
       "         0.4661042 ,  0.63517156]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_test)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms Model Creation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, make_scorer\n",
    "\n",
    "auc = make_scorer(roc_auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFModel = RandomForestClassifier(random_state = 123)\n",
    "\n",
    "criterion = ['gini', 'entropy']\n",
    "max_depth = [1, 3, 5, None]\n",
    "max_features = ['auto', 'sqrt', 'log2']\n",
    "\n",
    "grid = GridSearchCV(estimator = RFModel, scoring = auc, param_grid = dict(criterion = criterion, max_depth = max_depth, max_features = max_features))\n",
    "# default setting already uses the desires 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(random_state=123),\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': [1, 3, 5, None],\n",
       "                         'max_features': ['auto', 'sqrt', 'log2']},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149335382214336\n",
      "{'criterion': 'gini', 'max_depth': 5, 'max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=123)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RFModel = RandomForestClassifier(criterion = 'gini', max_depth = None, max_features = 'auto', random_state = 123)\n",
    "\n",
    "RFModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBModel = GaussianNB()\n",
    "\n",
    "var_smoothing = [1e-10, 3e-10, 1e-9, 3e-9]\n",
    "\n",
    "grid = GridSearchCV(estimator = NBModel, scoring = auc, param_grid = dict(var_smoothing = var_smoothing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=GaussianNB(),\n",
       "             param_grid={'var_smoothing': [1e-10, 3e-10, 1e-09, 3e-09]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7795527743401245\n",
      "{'var_smoothing': 1e-10}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(var_smoothing=1e-10)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NBModel = GaussianNB(var_smoothing = 1e-10)\n",
    "\n",
    "NBModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRModel = LogisticRegression(random_state = 123)\n",
    "\n",
    "penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "max_iter = [300, 1000, 2000, 3000]\n",
    "\n",
    "grid = GridSearchCV(estimator = LRModel, scoring = auc, param_grid = dict(penalty = penalty, solver = solver, max_iter = max_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(random_state=123),\n",
       "             param_grid={'max_iter': [300, 1000, 2000, 3000],\n",
       "                         'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
       "                         'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag',\n",
       "                                    'saga']},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808791566681425\n",
      "{'max_iter': 300, 'penalty': 'l1', 'solver': 'saga'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=300, penalty='l1', random_state=123, solver='saga')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LRModel = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 300, random_state = 123)\n",
    "\n",
    "LRModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVMModel = SVC(random_state = 123)\n",
    "\n",
    "C = [0.1, 0.3, 1, 3]\n",
    "degree = [1, 2, 3, 4]\n",
    "kernel = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "grid = GridSearchCV(estimator = SVMModel, scoring = auc, param_grid = dict(C = C, degree = degree, kernel = kernel))\n",
    "# default setting already uses the desires 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(random_state=123),\n",
       "             param_grid={'C': [0.1, 0.3, 1, 3], 'degree': [1, 2, 3, 4],\n",
       "                         'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8164708548350861\n",
      "{'C': 0.3, 'degree': 1, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.3, degree=1, kernel='linear', probability=True, random_state=123)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVMModel = SVC(C = 0.3, degree = 1, kernel = 'linear', random_state = 123, probability = True)\n",
    "\n",
    "SVMModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNModel = KNeighborsClassifier()\n",
    "\n",
    "n_neighbors = [3, 5, 10, 30, 100]\n",
    "\n",
    "grid = GridSearchCV(estimator = KNNModel, scoring = auc, param_grid = dict(n_neighbors = n_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': [3, 5, 10, 30, 100]},\n",
       "             scoring=make_scorer(roc_auc_score))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.799089334040261\n",
      "{'n_neighbors': 30}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=30)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNNModel = KNeighborsClassifier(n_neighbors = 30)\n",
    "\n",
    "KNNModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics on Test Set\n",
    "\n",
    "Evaluation based on Metrics of:\n",
    "- accuracy\n",
    "- sensitivity \n",
    "- specificity \n",
    "- AUC\n",
    "- Brier score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, brier_score_loss, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that outputs all evaluation metrics on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(typeModel, _X_test, _y_test):\n",
    "    \n",
    "    y_preds = typeModel.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(_y_test, y_preds)    \n",
    "    AUC = roc_auc_score(_y_test, y_preds)    \n",
    "    brier_score = brier_score_loss(_y_test, y_preds)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(_y_test, y_preds).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)    \n",
    "    specificity = tn / (tn + fp)\n",
    "        \n",
    "    return accuracy, sensitivity, specificity, AUC, brier_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that outputs all evaluation metrics on test set only when model has confidence of >75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics_3_way(typeModel, _X_test, _y_test):\n",
    "    \n",
    "    y_pred_prob = typeModel.predict_proba(_X_test)\n",
    "\n",
    "    y_max_pred_prob = np.amax(y_pred_prob, 1)\n",
    "\n",
    "    y_pred = typeModel.predict(X_test)\n",
    "\n",
    "    y_max_pred_prob = np.expand_dims(y_max_pred_prob, axis = 1)\n",
    "    y_pred = np.expand_dims(y_pred, axis = 1)\n",
    "    y_test_copy = np.expand_dims(_y_test, axis = 1)\n",
    "\n",
    "    confidence_array = np.concatenate((y_max_pred_prob, y_pred, y_test_copy),axis = 1)\n",
    "\n",
    "    culled_confidence_array = confidence_array[confidence_array[:,0] > 0.75]\n",
    "\n",
    "    confident_y_preds = culled_confidence_array[:,1]\n",
    "    \n",
    "    confident_y_test = culled_confidence_array[:,2]\n",
    "    \n",
    "    accuracy = accuracy_score(confident_y_test, confident_y_preds)    \n",
    "    AUC = roc_auc_score(confident_y_test, confident_y_preds)    \n",
    "    brier_score = brier_score_loss(confident_y_test, confident_y_preds)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(confident_y_test, confident_y_preds).ravel()\n",
    "    \n",
    "    sensitivity = tp / (tp + fn)    \n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    coverage = confident_y_preds.shape[0] / y_pred.shape[0]\n",
    "        \n",
    "    return accuracy, sensitivity, specificity, AUC, brier_score, coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8247126436781609 0.7804878048780488 0.8641304347826086 0.8223091198303287 0.1752873563218391\n"
     ]
    }
   ],
   "source": [
    "RF_accuracy, RF_sensitivity, RF_specificity, RF_AUC, RF_brier_score = eval_metrics(RFModel, X_test, y_test)\n",
    "\n",
    "print(RF_accuracy, RF_sensitivity, RF_specificity, RF_AUC, RF_brier_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9195979899497487 0.8681318681318682 0.9629629629629629 0.9155474155474156 0.08040201005025126 0.5718390804597702\n"
     ]
    }
   ],
   "source": [
    "RF_accuracy_3_way, RF_sensitivity_3_way, RF_specificity_3_way, RF_AUC_3_way, RF_brier_score_3_way, RF_coverage = eval_metrics_3_way(RFModel, X_test, y_test)\n",
    "\n",
    "print(RF_accuracy_3_way, RF_sensitivity_3_way, RF_specificity_3_way, RF_AUC_3_way, RF_brier_score_3_way, RF_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7701149425287356 0.8109756097560976 0.7336956521739131 0.7723356309650053 0.22988505747126436\n"
     ]
    }
   ],
   "source": [
    "NB_accuracy, NB_sensitivity, NB_specificity, NB_AUC, NB_brier_score = eval_metrics(NBModel, X_test, y_test)\n",
    "\n",
    "print(NB_accuracy, NB_sensitivity, NB_specificity, NB_AUC, NB_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7940199335548173 0.8461538461538461 0.7468354430379747 0.7964946445959105 0.2059800664451827 0.8649425287356322\n"
     ]
    }
   ],
   "source": [
    "NB_accuracy_3_way, NB_sensitivity_3_way, NB_specificity_3_way, NB_AUC_3_way, NB_brier_score_3_way, NB_coverage = eval_metrics_3_way(NBModel, X_test, y_test)\n",
    "\n",
    "print(NB_accuracy_3_way, NB_sensitivity_3_way, NB_specificity_3_way, NB_AUC_3_way, NB_brier_score_3_way, NB_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logisitic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8103448275862069 0.7987804878048781 0.8206521739130435 0.8097163308589608 0.1896551724137931\n"
     ]
    }
   ],
   "source": [
    "LR_accuracy, LR_sensitivity, LR_specificity, LR_AUC, LR_brier_score = eval_metrics(LRModel, X_test, y_test)\n",
    "\n",
    "print(LR_accuracy, LR_sensitivity, LR_specificity, LR_AUC, LR_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8935185185185185 0.8936170212765957 0.8934426229508197 0.8935298221137077 0.10648148148148148 0.6206896551724138\n"
     ]
    }
   ],
   "source": [
    "LR_accuracy_3_way, LR_sensitivity_3_way, LR_specificity_3_way, LR_AUC_3_way, LR_brier_score_3_way, LR_coverage = eval_metrics_3_way(LRModel, X_test, y_test)\n",
    "\n",
    "print(LR_accuracy_3_way, LR_sensitivity_3_way, LR_specificity_3_way, LR_AUC_3_way, LR_brier_score_3_way, LR_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8132183908045977 0.7987804878048781 0.8260869565217391 0.8124337221633086 0.1867816091954023\n"
     ]
    }
   ],
   "source": [
    "SVM_accuracy, SVM_sensitivity, SVM_specificity, SVM_AUC, SVM_brier_score = eval_metrics(SVMModel, X_test, y_test)\n",
    "\n",
    "print(SVM_accuracy, SVM_sensitivity, SVM_specificity, SVM_AUC, SVM_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915 0.896551724137931 0.9292035398230089 0.9128776319804699 0.085 0.5747126436781609\n"
     ]
    }
   ],
   "source": [
    "SVM_accuracy_3_way, SVM_sensitivity_3_way, SVM_specificity_3_way, SVM_AUC_3_way, SVM_brier_score_3_way, SVM_coverage = eval_metrics_3_way(SVMModel, X_test, y_test)\n",
    "\n",
    "print(SVM_accuracy_3_way, SVM_sensitivity_3_way, SVM_specificity_3_way, SVM_AUC_3_way, SVM_brier_score_3_way, SVM_coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7729885057471264 0.7865853658536586 0.7608695652173914 0.773727465535525 0.22701149425287356\n"
     ]
    }
   ],
   "source": [
    "KNN_accuracy, KNN_sensitivity, KNN_specificity, KNN_AUC, KNN_brier_score = eval_metrics(KNNModel, X_test, y_test)\n",
    "\n",
    "print(KNN_accuracy, KNN_sensitivity, KNN_specificity, KNN_AUC, KNN_brier_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9064039408866995 0.9021739130434783 0.9099099099099099 0.9060419114766941 0.09359605911330049 0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "KNN_accuracy_3_way, KNN_sensitivity_3_way, KNN_specificity_3_way, KNN_AUC_3_way, KNN_brier_score_3_way, KNN_coverage = eval_metrics_3_way(KNNModel, X_test, y_test)\n",
    "\n",
    "print(KNN_accuracy_3_way, KNN_sensitivity_3_way, KNN_specificity_3_way, KNN_AUC_3_way, KNN_brier_score_3_way, KNN_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1.\n",
      " 1. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 0. 1. 1. 0.\n",
      " 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0.] 216 348\n"
     ]
    }
   ],
   "source": [
    "#testing code, not part of workflow\n",
    "\n",
    "\n",
    "y_pred_prob = LRModel.predict_proba(X_test)\n",
    "\n",
    "y_max_pred_prob = np.amax(y_pred_prob, 1)\n",
    "\n",
    "y_pred = LRModel.predict(X_test)\n",
    "\n",
    "y_max_pred_prob = np.expand_dims(y_max_pred_prob, axis = 1)\n",
    "y_pred = np.expand_dims(y_pred, axis = 1)\n",
    "y_test_copy = np.expand_dims(y_test, axis = 1)\n",
    "\n",
    "#print(y_pred_prob.shape)\n",
    "#print(y_max_pred_prob.shape)\n",
    "#print((y_pred).shape)\n",
    "#print(y_test.shape)\n",
    "\n",
    "confidence_array = np.concatenate((y_max_pred_prob, y_pred, y_test_copy),axis = 1)\n",
    "#print(confidence_array.shape)\n",
    "#print(confidence_array[0:20,:])\n",
    "\n",
    "culled_confidence_array = confidence_array[confidence_array[:,0] > 0.75]\n",
    "#print(culled_confidence_array.shape)\n",
    "#print(culled_confidence_array[0:20, :])\n",
    "\n",
    "confident_y_preds = culled_confidence_array[:,1]\n",
    "\n",
    "confident_y_test = culled_confidence_array[:,2]\n",
    "print(confident_y_preds, confident_y_preds.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
